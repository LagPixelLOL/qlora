export PATH=$PATH:/gscratch/zlab/artidoro/miniconda3/bin
source "/gscratch/zlab/artidoro/miniconda3/bin/activate"; conda activate "fp4pr"
module load cuda/11.6
WANDB_PROJECT=flan_adapters python train.py  --logging_steps 10  --save_strategy steps  --data_seed 42  --save_steps 500  --save_total_limit 40  --evaluation_strategy steps  --eval_dataset_size 1024  --max_eval_samples 1000  --per_device_eval_batch_size 4  --max_new_tokens 32   --dataloader_num_workers 3  --use_accelerate   --group_by_length   --logging_strategy steps  --remove_unused_columns False   --do_train   --do_eval   --do_mmlu_eval   --lora_r 64  --lora_alpha 16  --lora_modules all  --compress_statistics   --quant_type nf4  --bf16  --max_memory_MB 30000 --bits 4 --model_name_or_path /gscratch/zlab/llama/7B   --warmup_ratio 0.03  --lr_scheduler_type constant  --gradient_checkpointing   --dataset alpaca --source_max_len 384 --target_max_len 128 --per_device_train_batch_size 16 --gradient_accumulation_steps 1   --max_steps 10000  --eval_steps 1000  --learning_rate 0.0002  --adam_beta2 0.999  --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0  --output_dir /gscratch/zlab/artidoro/efficient-tuning/checkpoints/finetuning/llama/stanford_alpaca/test/fp4test  --seed 0 --cache_dir /gscratch/zlab/artidoro/efficient-tuning/.cache --optim paged_adamw_32bit